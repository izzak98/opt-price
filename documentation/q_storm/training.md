# qStorm Training Documentation

This document describes the training procedure for the qStorm option pricing model.

## Overview

qStorm training uses physics-informed neural networks (PINNs) to solve stochastic partial differential equations (SPDEs) for option pricing.

## Loss Function

The total loss combines multiple components:

```
L_total = L_residual + λ_p·L_payoff + λ_u·L_under + λ_o·L_over + λ_i·L_inequality
```

### 1. SPDE Residual Loss

Enforces the stochastic PDE:

```
∂V'/∂t' + (1/2)(S')²ρ²∂²V'/∂(S')² + rf·S'·∂V'/∂S' - rf·V' = 0
```

**Computation**:
1. Sample z ~ N(0, 1) internally in `calc_varrho`
2. Compute stochastic coefficient ρ = varrho(t', varpi_q)
3. Compute partial derivatives using automatic differentiation
4. Compute residual: `residual = ∂V'/∂t' + (1/2)(S')²ρ²∂²V'/∂(S')² + rf·S'·∂V'/∂S' - rf·V'`
5. Average over Monte Carlo samples: `L_residual = mean(residual²)`

**Monte Carlo Integration**: Multiple samples of z are averaged to handle stochasticity.

### 2. Terminal Payoff Loss

Enforces terminal boundary condition at expiration (t' = 0):

```
L_payoff = mean((V'(S', 0) - max(S' - 1, 0))²)
```

For call options, the payoff is max(S' - 1, 0) = max(S/K - 1, 0).

### 3. Lower Boundary Loss

Enforces lower boundary condition (S' = 0):

```
L_under = mean(V'(0, t')²)
```

At S' = 0, the option value should be 0.

### 4. Upper Boundary Loss

Enforces upper boundary condition (S' = S_max):

```
L_over = mean((V'(S_max, t') - (S_max - 1))²)
```

At high stock prices, the option value approaches S_max - 1.

### 5. Inequality Loss (American Options)

Enforces American option constraint:

```
L_inequality = mean(ReLU(max(S' - 1, 0) - V'(S', t')))
```

Ensures V'(S', t') ≥ max(S' - 1, 0) (option value ≥ intrinsic value).

## Training Procedure

### Data Sampling

Training samples are generated by `StormSampler`:

1. **Stock Prices**: Sample S' uniformly in [0, S_max]
2. **Time**: Sample t' uniformly in [min_time, max_time] days
3. **Risk-free Rate**: Sample rf uniformly in [rf_min, rf_max]
4. **Varphi Quantiles**: Sample from cached varphi quantile predictions

### Training Loop

```python
for epoch in range(epochs):
    # Sample training points
    sampled_data = sampler.sample(n_points)
    
    # Compute losses
    losses = trainer(model, sampled_data, taus, mc_samples)
    
    # Backpropagation
    total_loss = sum(losses.values())
    total_loss.backward()
    optimizer.step()
```

### Hyperparameters

- **Learning Rate**: Typically 1e-4 to 1e-3
- **Batch Size**: Number of sampled points per iteration
- **Monte Carlo Samples**: Number of z samples for residual loss (typically 10-100)
- **Loss Weights**: λ_p, λ_u, λ_o, λ_i (tuned for balance)

### Optimization

- **Optimizer**: Adam or AdamW
- **Scheduler**: Cosine annealing or step decay
- **Early Stopping**: Based on validation loss

## Boundary Point Sampling

Different types of training points:

1. **Interior Points**: Random (S', t') pairs for SPDE residual
2. **Terminal Points**: t' = 0 for payoff loss
3. **Lower Boundary**: S' = 0 for lower boundary loss
4. **Upper Boundary**: S' = S_max for upper boundary loss
5. **Inequality Points**: Random points for American constraint

## Automatic Differentiation

Partial derivatives are computed using `torch.autograd.grad`:

```python
dV_dt = torch.autograd.grad(V, t_prime, create_graph=True)[0]
dV_dS = torch.autograd.grad(V, S_prime, create_graph=True)[0]
d2V_dS2 = torch.autograd.grad(dV_dS, S_prime, create_graph=True)[0]
```

`create_graph=True` enables second-order derivatives.

## Stochastic Term Handling

The stochastic coefficient ρ is computed from:
- Varphi quantiles (learned return distribution)
- Random sample z ~ N(0, 1) (generated internally)

This makes the SPDE stochastic and requires Monte Carlo averaging.

## Model Files

- **Training Script**: `q_storm/qstorm_train.py`
- **Trainer**: `q_storm/StormTrainer.py`
- **Sampler**: `q_storm/StormSampler.py`
- **Stochastic Coefficient**: `q_storm/storm_utils/varrho.py`

## References

See [MATHEMATICAL_FORMULATION.md](../../MATHEMATICAL_FORMULATION.md) for complete mathematical details.

