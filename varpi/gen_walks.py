import numpy as np
import json
import os
import pickle
from tqdm import tqdm
import torch
from torch.utils.data import Dataset
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

from utils.dist_utils import generate_smooth_pdf
from varpi.wasserstein_min import get_best_lstm_pdf_params

with open('config.json', encoding='utf-8') as f:
    CONFIG = json.load(f)


def generate_random_walk(grid, cdf, T):
    random_uniform_samples = np.random.uniform(0, 1, T)
    predicted_samples = np.interp(random_uniform_samples, cdf, grid)
    predicted_samples = np.concatenate([[0], predicted_samples])
    discrete_random_walk = np.cumsum(predicted_samples)
    return discrete_random_walk


def gen_quantiles(grid, cdf, num_samples, T, taus):
    # Vectorized generation of all random walks at once
    random_uniform_samples = np.random.uniform(0, 1, (num_samples, T))
    # Vectorized interpolation: np.interp works element-wise on arrays
    predicted_samples = np.interp(random_uniform_samples, cdf, grid)

    # Prepend zeros and compute cumulative sums
    zeros = np.zeros((num_samples, 1))
    walks = np.concatenate([zeros, predicted_samples], axis=1)
    walks = np.cumsum(walks, axis=1)

    # Compute quantiles for each time step (matching original: indices 0 to T-1)
    quantiles = [np.array([0]*len(taus))]
    for i in range(1, T):
        sub_quantiles = np.quantile(walks[:, i], taus)
        quantiles.append(sub_quantiles)
    quantiles = np.array(quantiles)
    return quantiles


def load_quants(t):
    """
    Load quantile predictions for time horizon T.

    Loads quantile predictions generated by generate_varphi_quants.py
    for a specific time horizon T (days).

    Args:
        t: Time horizon in days (15-30)

    Returns:
        Dictionary with asset classes as keys, each containing:
        - "all_pred_quantiles": np.array of quantile predictions
        - "observed_returns": np.array of observed returns
        - "future_returns": np.array of future returns
    """
    folder_name = "stored_quants"
    file_name = f"quantiles_{t}.pkl"  # Matches naming from generate_varphi_quants.py
    file_path = os.path.join(folder_name, file_name)

    if not os.path.exists(file_path):
        raise FileNotFoundError(
            f"Quantile predictions not found at {file_path}. "
            f"Please run generate_varphi_quants.py first to generate quantiles for T={t}."
        )

    with open(file_path, "rb") as f:
        quants = pickle.load(f)
    return quants


class WalkDataSet(Dataset):
    def __init__(self):
        self.X = []
        self.t = []
        self.T = []
        self.Y = []
        self.max_T = 0

    def ingest(self, varphi_quants, varpi_quants, T):
        # varphi shape (num_quantiles)
        # varpi shape (T, num_quantiles)
        # T is the number of time steps (max 30)
        if self.max_T < T:
            self.max_T = T
        for t in range(T):
            self.X.append(varphi_quants)
            self.t.append(t)
            self.T.append(T)
            self.Y.append(varpi_quants[t])

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        X = self.X[idx]
        X = torch.tensor(X, dtype=torch.float32)

        T = self.T[idx]

        t = self.t[idx]
        t = torch.tensor([t/T], dtype=torch.float32)

        T = torch.tensor([T/30], dtype=torch.float32)

        Y = self.Y[idx]

        return X, t, T, Y


def process_single_quantile(args):
    """Worker function for parallel processing of a single quantile."""
    varphi_quant, t, taus, best_pdf_params = args
    grid, _, cdf = generate_smooth_pdf(varphi_quant, taus, **best_pdf_params)
    varpi_quants = gen_quantiles(grid, cdf, 10000, t, taus)
    return varphi_quant, varpi_quants, t


def get_walk_dataset(num_workers=None):
    """
    Generate walk dataset with parallelization.

    Args:
        num_workers: Number of parallel workers. If None, uses CPU count - 1.
    """
    if num_workers is None:
        num_workers = max(1, mp.cpu_count() - 1)

    already_exists = os.path.exists("walk_dataset.pkl")
    if already_exists:
        with open("walk_dataset.pkl", "rb") as f:
            dataset = pickle.load(f)
        if dataset.max_T == 30:
            return dataset
        else:
            starting_t = dataset.max_T + 1
    else:
        starting_t = 15

    best_pdf_params = get_best_lstm_pdf_params(None)
    taus = CONFIG["general"]["quantiles"]
    taus = np.array(taus)
    dataset = WalkDataSet()

    # Prepare all tasks
    tasks = []
    for t in range(starting_t, 31):
        quants = load_quants(t)
        for asset_class in quants.keys():
            varphi_quants = quants[asset_class]["all_pred_quantiles"]
            for varphi_quant in varphi_quants:
                tasks.append((varphi_quant, t, taus, best_pdf_params))

    # Process tasks in parallel
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(process_single_quantile, task) for task in tasks]

        # Process results as they complete with progress bar
        for future in tqdm(as_completed(futures), total=len(futures), desc="Generating Walks"):
            varphi_quant, varpi_quants, t = future.result()
            dataset.ingest(varphi_quant, varpi_quants, t)

    # Save dataset after each time horizon (for safety)
    with open("walk_dataset.pkl", "wb") as f:
        pickle.dump(dataset, f)

    return dataset


if __name__ == "__main__":
    dataset = get_walk_dataset()
    print(len(dataset))
